{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and create a new SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the country CSV file into an RDD\n",
    "country_lines = sqlContext.textFile('file:///home/cloudera/Downloads/big-data-3/final-project/country-list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each line into a pair of words\n",
    "pair_words = country_lines.flatMap(lambda line : line.split(\"\\n\"))\n",
    "pair_words.take(5) # pair_words.collect() or pair_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each pair of words into a tuple\n",
    "country_tuples = pair_words.map(lambda word : (word.split(\",\")[0], word.split(\", \")[1]))\n",
    "country_tuples.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame, look at schema and contents\n",
    "countryDF = sqlContext.createDataFrame(country_tuple, [\"country\", \"code\"])\n",
    "countryDF.printSchema()\n",
    "countryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_countryDF = countryDF.toPandas()\n",
    "pandas_countryDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tweets CSV file into RDD of lines\n",
    "tweet_texts = sc.textFile(\"file:///home/cloudera/Downloads/big-data-3/final-project/Tweets.csv\")\n",
    "tweets_texts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data: some tweets are empty. Remoce the empty tweets using filer()\n",
    "clean_tweet_texts = tweet_texts.filter(lambda x : len(x) > 0)\n",
    "clean_tweet_texts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform WordCount on the cleaned tweet texts.\n",
    "tweet_words = clean_tweet_texts.flatMap(lambda line : line.split(\" \"))\n",
    "tweet_tuples = tweet_words.map(lambda word : (word, 1))\n",
    "word_counts = tweet_tuples.reduceByKey(lambda a,b : (a + b))\n",
    "word_counts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of tweet word counts\n",
    "tweetDF = sqlContext.createDataFrame(word_counts, [\"word\", \"count\"])\n",
    "tweetDF.printSchema()\n",
    "tweetDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join country and Tweet DataFrames on country and word name\n",
    "from pyspark.sql.funtions import col\n",
    "joinedDF = countryDF.alias('c').join(tweetDF.alias('t'), col('c.country') == col('t.word')).select(col('c.code'), col('c.country'), col('t.word'))\n",
    "joinedDF.printSchema()\n",
    "joinedDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: number of distinct countries mentioned\n",
    "joinedDF.count(), joinedDF.select('code').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: number of countries mentioned in tweets.\n",
    "from pyspark.sql.funtions import sum\n",
    "joinedDF.agg(sum(\"count\")).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Top three countries and their counts\n",
    "from pyspark.sql.funtions import desc\n",
    "descSorted = joinedDF.sort(desc(\"count\"))\n",
    "descSorted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Counts for Wales, Iceland and Japan.\n",
    "selectedDF = joinedDF.where((col(\"country\") == \"Wales\") | (col(\"country\") == \"Iceland\") | (col(\"country\") == \"Japan\")).sort(desc(\"count\")))\n",
    "selectedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "joinedDF.agg(avg(\"count\")).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wite counts to text file in HDFS\n",
    "word_counts.coalesce(1).saveAsTextFile('hdfs:/user/cloudera/wordcount/outputDir') # coalese() method combines all the RDD partition into a single partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
